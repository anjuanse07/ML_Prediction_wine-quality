# -*- coding: utf-8 -*-
"""Proyek_Pertama_ML_Terapan_V3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xecokqLSUYBJufNJmCRH8Hx1fOqznayX

Proyek Pertama Machine Learning Terapan

Nama : Sean Julius Lase

SIB Email : M011X0096@dicoding.org

Domisili : Jatinangor, Sumedang, Jawa Barat

Tahap pertama, Mendeklarasikan atau import library yang dibutuhkan dalam proses pembuatan proyek. semua import disatukan dalam 1 cell agar saat cell di compile program berjalan lebih rapih dan tidak mengganggu algoritma lain.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import  OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostRegressor ,RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense ,LSTM, Normalization
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.optimizers import Adam

"""Selanjutnya membuat syntax untuk memanggil dataset yang akan digunakan (data loading). dataset diambil dari kaggle dengan link : https://www.kaggle.com/datasets/yasserh/wine-quality-dataset , dengan search engine : regression datasets. Dataset yang digunakan adalah dataset tentang kualitas dari Wine. dataset diupload ke google drive pribadi, dan dipanggil dengan mendeklarasikan ulang path nya pada variable yang telah dibuat (url-data), dengan menggunakan lib pandas untuk membaca dataset dalam bentuk .csv"""

url = '/content/drive/MyDrive/Dataset/WineQT.csv'
data = pd.read_csv(url)

"""Pendeklarasian variabel data untuk mencari tahu informasi tentang baris x kolom yang dimiliki oleh dataset, dan diketahui ada 1143 baris x 13 kolom yang masing masing nilainya di tunjukan seperti hasil output dibawah."""

data

"""Selanjutnya masuk ke tahap EDA atau Exploratory Data Analysis untuk menganalisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data. pada bagian ini adalah deskripsi variabel yang didapat dari informasi yang disediakan pada kaggle. 

data.info() dideklarasikan untuk mencari tahu tipe data yang ada pada dataset, diketahui bahwa ada 11 data tipe float64 dan 2 data tipe int64, semuanya adalah data numerikal ,tidak ada kategorikal.
"""

data.info()

"""Selanjutnya adalah menggunakan fungsi .drop() untuk membuang kolom yang tidak dibutuhkan dari dataset, pada kali ini yang di drop adalah kolom ID, karena dari hasil pengamatan ,kolom ini tidak akan memberikan pengaruh signifikan. kemudian untuk mengecek apakah sudah terhapus bisa dengan mendeklarasikan kembali variabel "data"."""

data.drop('Id', inplace=True, axis=1)
data

"""Kemudian memanggil fungsi describe() yang memberikan informasi statistik pada masing-masing kolom, dimana Count  adalah jumlah sampel pada data. Mean adalah nilai rata-rata. Std adalah standar deviasi. Min yaitu nilai minimum setiap kolom.  25% adalah kuartil pertama. Kuartil adalah nilai yang menandai batas interval dalam empat bagian sebaran yang sama. 50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah). 75% adalah kuartil ketiga. Max adalah nilai maksimum."""

data.describe()

"""untuk memastikan kembali tidak ada data kosong maka digunakan fungsi .isna().sum() digunakan untuk mencari jumlah data yang tidak memiliki nilai, atau data kosong (NULL/NaN). dan dapat dilihat dari hasil output dibawah, bahwa tidak ada data kosong atau missing value."""

data.isna().sum()

"""Selanjutnya masuk ketahap untuk menangani outliers sebagai tahap bagian dari EDA. untuk melihat secara langsung outliers yang ada pada tiap data bisa dengan teknik visualisasi data (boxplot) menggunakan fungsi sns.boxplot(). Kemudian, menangani outliers dengan teknik IQR method atau Inter Quartile Range. boxplot menunjukkan ukuran lokasi dan penyebaran, serta memberikan informasi tentang simetri dan outliers."""

sns.boxplot(x=data['total sulfur dioxide'])

sns.boxplot(x=data['pH'])

"""Setelah beberapa contoh outliers diatas, untuk menangani nya berdasarkan uraian sebelumnya adalah dengan menggunakan IQR. metode IQR mengidentifikasi outlier yang berada di luar Q1 dan Q3 (kuartil 1 dan kuartil 3). Nilai apa pun yang berada di luar batas ini dianggap sebagai outlier. setelah kode dideklarasikan data akan mengambil 1.5 nilai IQR dibawah Q1 sampai 1.5 nilai IQR diatas Q3.
hasil dari penanganan outliers dapat dilihat dari data.shape() yang menghitung ulang jumlah baris x kolom yang sudah bersih. (834 baris x 12 kolom)
"""

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR=Q3-Q1
data=data[~((data<(Q1-1.5*IQR))|(data>(Q3+1.5*IQR))).any(axis=1)]
 

data.shape

"""Selanjutnya, dilakukan proses analisis data dengan menggunakan teknik Univariate EDA. yang harus dilakukan adalah membagi dataset menjadi numerikal dan kategorikal, namun dikarenakan semua tipe data yang digunakan tidak ada yang bertipe kategorikal, maka hanya ada numerikal yang di deklarasikan."""

numerical_features = ['fixed acidity'	,'volatile acidity',	'citric acid'	,
                      'residual sugar',	'chlorides'	,'free sulfur dioxide',	'total sulfur dioxide',
                      'density',	'pH',	'sulphates',	'alcohol',	'quality']

"""Untuk melihat fitur numerik, dideklarasikan fungsi histogram untuk melihat histogram masing-masing fiturnya agar dapat dianalisa."""

data.hist(bins=50, figsize=(20,15))
plt.show()

"""Masih dalam tahapan EDA, setelah melalui univariate EDA, sekarang adalah Multivariate EDA yang menunjukkan hubungan antara dua atau lebih variabel pada data. Multivariate EDA yang menunjukkan hubungan antara dua variabel biasa disebut sebagai bivariate EDA. Untuk mengamati hubungan antara fitur numerik,akan digunakan fungsi pairplot(). Fungsi pairplot dari library seaborn menunjukkan relasi pasangan dalam dataset. Data yang memiliki hubungan positif satu sama lain maka hasil plot akan menunjukan grafik naik ke arah kanan atas. jika tidak maka data akan menunjukan grafik menurun ke kanan bawah, dan apabila tidak ada korelasi sama sekali maka pola data tidak akan terlihat.

"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(data, diag_kind = 'kde')

"""Tahap selanjutnya dalam EDA adalah dengan menggunakan fungsi .corr() untuk mencari korelasi sebaran data pada grafik pairplot di atas, Korelasi pada fitur tampak dari adanya pola pada sebaran data. Sebaran data acak merupakan indikasi korelasi yang lemah (atau tidak ada korelasi sama sekali). Arah korelasi antara dua variabel bisa bernilai positif (nilai kedua variabel cenderung meningkat bersama-sama) maupun negatif (nilai salah satu variabel cenderung meningkat ketika nilai variabel lainnya menurun)"""

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr().round(2)
 
# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""dengan melihat nilai yang tertera pada correlation matriks yang sudah dibuat, dapat dianalisis dan diambil keputusan bahwa ada beberapa variabel yang tidak memiliki korelasi yang cukup kuat satu sama lain, sehingga diputuskan untuk dibuang dengan fungsi .drop()"""

data.drop(['pH','free sulfur dioxide', 'total sulfur dioxide', 'citric acid','chlorides','volatile acidity'], inplace=True, axis=1)

"""Lalu dilakukan pengecekan kembali matriks korelasi yang sudah di minimalisasi atas nilai korelasi yang buruk. dan berikut adalah hasil akhir dari matriks korelasi yang sudah disesuaikan kembali."""

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr().round(2)
 

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Masuk ke tahap persiapan data. karena tidak ada data kategorikal maka tidak diperlukan penggunaaan fungsi encoding, serta tidak ada Reduksi Dimensi dengan PCA. sehingga dilanjutkan dengan memisah data untuk training dan test dengan fungsi train_test_split. dengan perbandingan 1:9 ,X berisi semua kolom yang dijadikan input, sedangkan y menjadi target dari x yaitu kualitas."""

X = data.drop(["quality"],axis =1)
y = data["quality"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

"""Masih dalam tahap persiapan data , berikut adalah code untuk melihat total sampel yang digunakan dalam pelatihan. total dataset 834, yang digunakan sebagai data latih 750, dan data tes 84."""

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""Selanjutnya adalah melakukan standarisasi. algoritma machine learning memiliki performa lebih baik dan konvergen lebih cepat ketika dimodelkan pada data dengan skala relatif sama atau mendekati distribusi normal. Proses scaling dan standarisasi membantu untuk membuat fitur data menjadi bentuk yang lebih mudah diolah oleh algoritma. """

numerical_features =  ['fixed acidity','residual sugar', 'density',
                       'sulphates',	'alcohol']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""Masuk kedalam tahap Pengembangan model, dimana pada tahap ini saya menggunakan 3 buah model seperti yang dicontohkan dalam submodul study case pertama : predictive Analysis, namun terlebih dahulu menyiapkan data frame untuk analisis ketiga model tersebut."""

models = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""Selanjutnya, untuk melatih data dengan KNN, bisa dengan menuliskan kode dibawah. menggunakan k = 10 tetangga dan metric Euclidean untuk mengukur jarak antara titik. Algoritma KNN menggunakan ‘kesamaan fitur’ untuk memprediksi nilai dari setiap data yang baru. Dengan kata lain, setiap data baru diberi nilai berdasarkan seberapa mirip titik tersebut dalam set pelatihan."""

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)
 
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""Selanjutnya, dengan model kedua membuat variabel RF dan memanggil RandomForestRegressor dengan beberapa nilai parameter. parameter yang digunakan adalah, n_estimator: jumlah trees (pohon) di forest. max_depth: kedalaman atau panjang pohon.  random_state: digunakan untuk mengontrol random number generator yang digunakan.  n_jobs: jumlah job (pekerjaan) yang digunakan secara paralel. n_jobs=-1 artinya semua proses berjalan secara paralel."""

RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)
 
models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""pada model ini, digunakan model ada boost regressor, yang dimana jika semua kasus dalam data latih memiliki weight atau bobot yang sama. maka Pada model ini di setiap tahapan, model akan memeriksa apakah observasi yang dilakukan sudah benar, Bobot yang lebih tinggi kemudian diberikan pada model yang salah sehingga mereka akan dimasukkan ke dalam tahapan selanjutnya."""

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)                             
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""sebelum menghitung nilai MSE dalam model, diperlukan melakukan proses scaling fitur numerik pada data uji. setelah model selesai dilatih dengan 3 algoritma, yaitu KNN, Random Forest, dan Adaboost, perlu dilakukan proses scaling terhadap data uji. Hal ini harus dilakukan agar skala antara data latih dan data uji sama dan bisa melakukan evaluasi."""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""Kemudian mengevaluasi masing masing model yang telah dibuat dengan menggunakan MSE atau mean squarred error. dan hasilnya bisa dilihat seperti pada output dibawah."""

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])
 
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}
 
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
 
mse

"""Agar dapat lebih mudah dilihat, Mendeklarasikan fungsi figure, plt.subplots(). disediakan dalam bentuk grafik bar atas evaluasi model yang dilakukan. dan dapat dilihat bahwa model Boosting memberikan nilai error dan memiliki hasil evaluasi yang paling kecil dibandingkan yang lainnya."""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Untuk menguji hasil prediksi model yang telah dibuat, bisa dengan melakukan prediksi menggunakan beberapa data dari data test. dengan menggunakan code dibawah. dipanggil 10 data pertama untuk menguji hasil prediksi."""

prediksi = X_test.iloc[:10].copy()
pred_dict = {'y_true':y_test[:10]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

"""Selanjutnya, peserta mencoba model lain yaitu dengan menggunakan model sekuensial dalam memprediksi sistem regresi penentuan kualitas wine atau anggur. dengan menggunakan fungsi LSTM, Normalization, dan beberapa hidden layer. menggunakan fungsi aktivasi relu, dengan total 9 layer."""

tf.random.set_seed(42)
model_s = tf.keras.models.Sequential([
      tf.keras.layers.LSTM(256, return_sequences=True, input_shape= (X_train.shape[1], 1)),
      tf.keras.layers.Normalization(),
      tf.keras.layers.Dense(1024, activation='relu'),
      tf.keras.layers.Dropout(0.4),
      tf.keras.layers.Dense(512, activation='relu'),
      tf.keras.layers.Dense(256, activation='relu'),
      tf.keras.layers.Dense(1)
])

"""Setelah model sekuensial dibuat, selanjutnya adalah mendefinisikan optimizer yang diperlukan, pada kali ini digunakan keras optimizer SGD. Penurunan gradien stokastik (sering disingkat SGD) adalah metode iteratif untuk mengoptimalkan fungsi objektif dengan sifat kehalusan yang sesuai untuk menangani noisy probelm pada model. Kemudian melakukan compiling model dengan metriks mae dan variabel loss mse."""

optimizer = tf.keras.optimizers.SGD(learning_rate=1.000e-02, momentum=0.9)
model_s.compile(optimizer=optimizer, loss='mean_squared_error',metrics=['mae'])

"""Dalam menggunakan metrik mae diperlukan nilai minimum mae dengan nilai target adalah 10% dari label/target menggunakan perhitungan seperti berikut. Nilai minimum mae dibutuhkan sebagai tolak ukur model untuk memiliki akurasi yang baik."""

minMAE = (data['quality'].max() - data['quality'].min()) * 0.1
minMAE

"""Mendeklarasikan fungsi Callback yang berguna untuk menghentikan proses training apabila nilai mae yang diperoleh pada saat training sudah berada dibawah nilai minMAE yaitu mae yang telah dihitung pada algoritma diatas."""

class Callbacks(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
        if(logs.get('mae') < minMAE):
            print("\nmae < 10%!") 
            self.model.stop_training = True 
callbacks = Callbacks()

"""Selanjutnya mengecek arsitektur model yang dibuat dengan menggunakan fungsi summary(). dari model didapat terdapat 782.849 parameter yang dilatih."""

model_s.summary()

"""Selanjutnya melatih model yang dibuat dengan menyimpan hasil dari training yang dilakukan pada variabel history. training dilakukan sebanyak 5000 epoch dengan fungsi callback dan mendapatkan output seperti yang dapat dilihat dibawah."""

history = model_s.fit(X_train,y_train, epochs=5000, verbose=1 ,callbacks = [callbacks] )

"""Terakhir adalah dengan melakukan plotting pada model sekuensial yang telah dibuat berdasarkan nilai mae dan loss yang dihasilkan selama proses training. hasil yang didapat cukup baik mengingat nilai loss dan mae sudah berada dibawah nilai 10% yang ditentukan."""

# Membuat plot akurasi model
plt.figure(figsize=(10,4))
plt.plot(history.history['mae'])
plt.title('model mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.grid(True)
plt.show()

print()

# Membuat plot loss model
plt.figure(figsize=(10,4))
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.grid(True)
plt.show()